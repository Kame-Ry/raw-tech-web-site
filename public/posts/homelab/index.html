<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>The Homelab Project | RAW-TECH</title>
<meta name="keywords" content="docker, plex, proxmox, tech, linux">
<meta name="description" content="A technical journey through building a Plex-focused homelab from the ground up, covering architecture, automation, monitoring, and the philosophy behind it all.">
<meta name="author" content="Ryan Witts">
<link rel="canonical" href="http://localhost:1313/posts/homelab/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.fa382cd77ccc05462752d6a59c1e87f69503c45600807ebd4b65443726b7f475.css" integrity="sha256-&#43;jgs13zMBUYnUtalnB6H9pUDxFYAgH69S2VENya39HU=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/homelab/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="http://localhost:1313/posts/homelab/">
  <meta property="og:site_name" content="RAW-TECH">
  <meta property="og:title" content="The Homelab Project">
  <meta property="og:description" content="A technical journey through building a Plex-focused homelab from the ground up, covering architecture, automation, monitoring, and the philosophy behind it all.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-05-17T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-05-17T00:00:00+00:00">
    <meta property="article:tag" content="Docker">
    <meta property="article:tag" content="Plex">
    <meta property="article:tag" content="Proxmox">
    <meta property="article:tag" content="Tech">
    <meta property="article:tag" content="Linux">
    <meta property="og:image" content="http://localhost:1313/images/homelab/homelab.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://localhost:1313/images/homelab/homelab.png">
<meta name="twitter:title" content="The Homelab Project">
<meta name="twitter:description" content="A technical journey through building a Plex-focused homelab from the ground up, covering architecture, automation, monitoring, and the philosophy behind it all.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "The Homelab Project",
      "item": "http://localhost:1313/posts/homelab/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "The Homelab Project",
  "name": "The Homelab Project",
  "description": "A technical journey through building a Plex-focused homelab from the ground up, covering architecture, automation, monitoring, and the philosophy behind it all.",
  "keywords": [
    "docker", "plex", "proxmox", "tech", "linux"
  ],
  "articleBody": "Why I Built This Homelab Modern streaming culture is exhausting. Content is fragmented across too many services, each demanding money for one or two shows or films people actually care about. It’s frustrating to pay for access only to be served bloated catalogs, Marvel filler arcs, or algorithm-bait reality TV.\nSelf-hosted media libraries have emerged as a response to that, a way to curate a meaningful, ad-free experience. They’re often built around content that’s hard to access legally: older titles, niche genres, or region-locked releases. For many, setting up something like Plex is less about cost-saving and more about taking control.\nMedia self-hosting can be a form of protest.\nIt pushes back against a corporate model that prioritises subscriptions and ad revenue over storytelling. When content resonates, real fans often support it directly, through merchandise, physical media, or word of mouth, not passive monthly payments.\nThat’s why I started building my own infrastructure. It began as a simple setup, Plex on my PC with a few folders, but quickly evolved. I wanted automation, then Docker, then Linux, then monitoring, then a Minecraft server for friends.\nNow, it’s a full-blown homelab. Virtualised, containerised and automated.\nEvolution of the Homelab Stage Details Seed Plex on Windows. Manual file adds. Painful. Sprout Plex + Sonarr + Radarr on Windows. Messy file paths. Sapling Moved to Proxmox VM with Debian 12. Full CLI control. Tree Docker stack: VPN-tunneled qBittorrent, smart automation, live dashboard, Discord alerts. Each stage taught me valuable lessons, refined my approach, and ultimately resulted in the reliable, fully automated homelab.\nPhase 0: The Plan “I’m gonna make a homelab you can’t refuse.”\nBefore touching a single cable or burning an ISO, I sat down and mapped out what I actually wanted from this project. Not just “install Plex,” but an intentional, modular stack that could evolve over time without collapsing under its own weight.\nSo I split the entire journey into phases, each with a clear goal, checklist, and rollback plan if things went sideways:\nPhase Breakdown Phase Goal Phase 0: The Plan Define requirements, map out architecture, break the build into logical stages Phase 1: The Hardware Physically assemble and verify the server; install/upgrade components and prep BIOS Phase 2: The Operating System Install Proxmox VE and configure the VM that’ll host all containers Phase 3: The Docker \u0026 Containers Build the entire stack — Plex, torrenting, automation, and more Phase 4: The Network Harden the network, configure VPNs, firewalls, subnets, and remote access Phase 5: The Monitoring Add Uptime Kuma, dashboards, alerts, logging, and uptime recovery tools Phase 6: The Audit Final checklist, cleanup, disaster recovery documentation, and postmortem Guiding Principles These were the rules I forced myself to follow at every step:\nSelf-contained: No reliance on third-party clouds. My server, my data. Recoverable: Snapshot, and document recovery paths for every service. Modular: Everything must run in Docker with isolated volumes, configs, and environment files. Secure: No exposed ports unless explicitly needed. VPN tunnel preferred. Minimal surface area. Documented: If it’s not written down, it doesn’t exist. Markdown logs or it didn’t happen. Observable: If something breaks, ensure it’s visible Core Service List Here’s the minimum I scoped out for v1 of the homelab:\nPlex: The heart of the project qBittorrent + Gluetun: Torrents tunneled through VPN Sonarr / Radarr / Prowlarr: Automated content acquisition Tautulli: Usage tracking and watched status Uptime Kuma: Monitor critical services Homepage: Self-hosted status/dashboard Nginx Proxy Manager: SSL and reverse proxy Minecraft: Because why not FileBrowser: Lightweight UI for browsing storage DuckDNS: Basic dynamic DNS for remote access Tailscale: Secure overlay VPN across devices Storage Strategy This was critical. I wasn’t going to throw media and config files on the same disk like a scrub.\nDrive Mount Path Purpose 120GB SSD Proxmox OS Host the hypervisor 1TB SSD (EXT4) /mnt/ssd-linux All Docker configs, volumes, metadata 2TB SSD (NTFS) /mnt/plex-media Hot storage for recently added media 8TB HDD (NTFS) /mnt/hdd Cold archive for watched or backup content Each one passed through cleanly to the Debian VM using scsiX raw device passthrough in Proxmox. ntfs-3g handles mount duties for the NTFS partitions in /etc/fstab.\nSuccess Criteria How I knew this project would be “done” (for now):\nPlex serves content instantly on LAN and remote (via Tailscale) Automation pulls, renames, and organises new content without manual input Services restart automatically after crash or reboot Monitoring alerts me if anything breaks — including the mover script Minecraft server is online, backed up, and doesn’t turn into a laggy mess Full disaster recovery is possible with only access to my external drives and Markdown docs That was the blueprint.\nPhase 1: The Hardware “Say hello to my little friend… the ProDesk.”\nThis phase was supposed to be simple: unbox, upgrade, install. In reality? It turned into a low-level boss fight with HP’s factory screws.\nBase System: HP ProDesk 400 G6 Mini I chose this machine for a few reasons:\nSmall form factor: Fits anywhere, whisper quiet Low power draw: Ideal for 24/7 uptime Surprisingly capable: 6-core, 12-thread CPU and 64GB RAM capacity Cheap second-hand: Plenty available from ex-corporate stock Hardware Spec \u0026 Upgrade Plan I wanted a future-proof homelab node that could run Proxmox and host multiple VMs + containers without breaking a sweat.\nComponent Model / Detail CPU Intel i5-10500T (6C/12T @ 2.3GHz) RAM 64GB DDR4 SODIMM (2×32GB Crucial 3200MHz CL22) Primary Disk 1TB Kingston NV2 Gen4 NVMe SSD Backup Boot 120GB Generic M.2 SSD (fallback boot volume) External #1 2TB WD SN770 (hot media) via USB-C External #2 8TB Seagate Expansion HDD (archive) via USB-A Enclosure USB-C NVMe enclosure (used for the 1TB \u0026 2TB) Everything went smoothly — until I tried to swap the internal SSD.\nThe Screw Incident The pre-installed 120GB M.2 SSD was locked in place with a factory-stripped screw. HP must’ve torqued it with the force of a thousand suns. I tried everything:\nRubber bands Precision extraction bits Superglue (desperation move) Swearing profusely Nothing worked. That screw wasn’t coming out without tearing the board.\nAt this point, I had two options:\nDestroy the 120GB SSD and force in the 1TB NVMe — risky and irreversible Leave the 120GB in place and install the 1TB externally in a USB-C NVMe enclosure I chose option 2. It wasn’t ideal — but it worked. And it let me move forward without bricking the hardware on day one.\nRevised Strategy Instead of running Proxmox off the fast 1TB NVMe, I flipped the plan:\nDevice Role 120GB SSD Host Proxmox OS (internal, bootable) 1TB SSD (EXT4) Docker volumes, configs, Plex metadata 2TB SSD (NTFS) mount for unwatched media 8TB HDD (NTFS) Cold storage for watched media / backups All external drives passed through to the docker-host VM using Proxmox raw device passthrough (scsiX: /dev/sdX,format=raw). This gave the VM native block-level access to each disk.\nCooling Considerations With an NVMe drive running full-time in a plastic USB-C enclosure, I added:\nLow-profile thermal pads inside the case I don’t want my main Docker volume drive thermal throttling mid-transcode.\nBIOS Tweaks To prep for virtualization and passthrough, I adjusted:\nVT-x and VT-d: Enabled Secure Boot: Disabled Legacy Boot: Disabled Boot Priority: Set to internal 120GB SSD What Worked / What Didn’t What Worked Well What Was a Nightmare Upgrading RAM to 64GB Swapping the internal SSD USB passthrough for Docker volumes Thermal throttling risk on NVMe over USB-C BIOS config for VT-x / VT-d HP’s anti-human screw design Modular drive setup with fstab NTFS quirks on cold storage drives Lessons from Phase 1 Always test drive mounts with UUID and fstab before assuming they’ll survive a reboot Avoid gluing anything inside your PC unless you hate yourself 64GB RAM might be overkill USB-C NVMe is surprisingly fast (~900MB/s) but nowhere near native Gen4 speeds With the hardware stable, drives mounted, and BIOS dialed in, I was finally ready to install the hypervisor and start spinning up VMs.\nPhase 2: The Operating System “Yer a wizard, Debian.”\nWith the hardware finally stable (screw rage behind me), it was time to give this machine a brain.\nThere were many ways I could’ve gone here; bare metal Docker, Windows Server, Unraid, TrueNAS… but I wanted:\nVirtualisation support for running multiple isolated environments A strong CLI and community Total control over storage, snapshots, and PCI passthrough There was only one option that hit all the marks without being bloated: Proxmox VE.\nWhy Proxmox? Free, open-source hypervisor Clean web UI + full CLI access Built-in support for raw disk passthrough Powerful snapshot/backup tools Debian-based Installation Checklist OS: Proxmox VE 8.1\nInstall Method: Flashed ISO via Rufus → Booted from USB\nTarget Disk: Internal 120GB SSD\nPost-Install Tasks:\nSet static IP to 192.168.0.x (reserved via router DHCP) Disable subscription nags (sed + APT patch) Rename node to homelab Enable remote SSH access Create non-root user for SSH # Confirm node config pveversion -v Proxmox Configuration Recap Setting Value Hostname homelab Management IP 192.168.0.X (static) Web UI https://192.168.0.X:8006 Subscription Popup Disabled VM Planning My philosophy here was “one VM to rule them all”, but only if that VM was solid. I didn’t want 15 micro-VMs doing niche tasks badly.\nPrimary VM: docker-host\nSpec Value OS Debian 12 (Bookworm, minimal install) vCPU 8 cores RAM 48GB Disk (internal) 48GB on Proxmox SSD Mounts 1TB EXT4 (/mnt/ssd-linux), 2TB NTFS (/mnt/plex-media), 8TB NTFS (/mnt/hdd) IP Address 192.168.0.X (static via DHCP) Minimal Debian Install Notes Debian was installed manually using the netinst ISO.\nInstalled only: OpenSSH server sudo curl, wget, git, htop, rsync ntfs-3g for media drives Docker CE + Docker Compose v2 (installed via apt repo) I avoided the Desktop Environment, Snap packages, or any auto-magic config. Keep it lean.\nsudo apt update \u0026\u0026 sudo apt upgrade -y sudo apt install docker.io docker-compose -y Drive Mounts via fstab Mounts were UUID-based and cleanly defined for permanence across reboots.\n# /etc/fstab entries (UUIDs redacted) UUID=xxxxxx /mnt/plex-media ntfs-3g defaults 0 0 UUID=yyyyyy /mnt/ssd-linux ext4 defaults 0 2 UUID=zzzzzz /mnt/hdd ntfs-3g defaults 0 0 All drives were passed through from Proxmox using raw disk passthrough (scsiX) to give the VM full block access.\nSanity Checks Before Moving On VM boots cleanly SSH access stable (ssh XXXX@docker-host) All 3 drives auto-mount and are writable docker ps and docker compose run fine Static IP reachable via LAN fstab doesn’t hang at boot /etc/hosts and /etc/hostname match UFW firewall installed and tested Observations from Phase 2 Good Decisions Could’ve Been Better Using Debian minimal install Should’ve pre-written fstab entries to avoid a reboot cycle SSH from day 1 Consider setting up earlier for convenience Single powerful VM Might need to split in future if resource contention appears Using EXT4 for Docker configs NTFS was not fun to script across Phase 3: The Docker \u0026 Containers “Dock Hard”\nWith Debian installed and the drives mounted, it was finally time to start building the real infrastructure. This is where the homelab went from “just a VM” to a full service stack.\nWhy Docker? Because I’m not interested in:\nManually setting up services Dealing with conflicting dependencies Running a script and praying it survives a reboot Docker gives me:\nIsolation: Every service lives in its own box Portability: Move everything by copying one folder Repeatability: Rebuild my entire stack in seconds Observability: Every container has logs, healthchecks, and restart policies And most importantly: Docker Compose lets me define the entire stack in one YAML file.\nFolder Structure All containers live under /mnt/ssd-linux/docker, which sits on the 1TB EXT4 SSD passed through to the VM. It’s clean, fast, and not affected by NTFS permission weirdness.\n/mnt/ssd-linux/\r└── docker/\r├── compose.yml\r├── plex/\r├── tautulli/\r├── qbittorrent/\r├── gluetun/\r├── sonarr/\r├── radarr/\r├── prowlarr/\r├── homepage/\r├── uptime-kuma/\r├── filebrowser/\r└── scripts/\rCore Stack: Docker Compose Services Here’s a trimmed snapshot of what my docker-compose.yml looked like during this phase:\nversion: \"3.9\" services: plex: image: lscr.io/linuxserver/plex network_mode: host volumes: - ./plex:/config - /mnt/plex-media:/movies environment: - PUID=1000 - PGID=1000 - VERSION=docker - TZ=Europe/London restart: unless-stopped gluetun: image: qmcgaw/gluetun cap_add: - NET_ADMIN devices: - /dev/net/tun environment: - VPN_SERVICE_PROVIDER=pia - VPN_TYPE=wireguard - SERVER_CITIES=Amsterdam volumes: - ./gluetun:/gluetun restart: unless-stopped qbittorrent: image: linuxserver/qbittorrent network_mode: \"service:gluetun\" depends_on: - gluetun environment: - WEBUI_PORT=8082 volumes: - ./qbittorrent:/config - /mnt/plex-media/downloads:/downloads restart: unless-stopped Services Deployed Service Port(s) Notes Plex 32400 Core media server. Runs in host mode for LAN discovery \u0026 DLNA. qBittorrent 8082 (via VPN) Web UI tunneled through Gluetun (PIA WireGuard). Gluetun — VPN container (PIA, WireGuard). Tunnel for torrent + indexers. Sonarr 8989 TV automation. Behind Gluetun VPN. Radarr 7878 Movie automation. Behind Gluetun VPN. Prowlarr 9696 Indexer manager. Behind Gluetun VPN. Tautulli 8181 Plex stats and activity tracker. Homepage 3005 Custom homelab dashboard. Internal only. Uptime Kuma 3001 Status monitoring. Internal only. FileBrowser 8085 Lightweight file manager. Internal only. DuckDNS — Dynamic DNS. Updates external IP for domain access. Minecraft 25565 Hosted LAN game server. Publicly exposed Nginx Proxy Manager 80 / 443 GUI reverse proxy. Tailscale — Private overlay network for remote SSH/management. FlareSolverr 8191 Captcha solver for Prowlarr. Routed via Gluetun. Netdata 19999 Real-time system monitoring. Internal access only. Portainer 9000 Docker management GUI. Internal only. VPN Enforcement (Gluetun) Torrenting is routed entirely through a VPN using Gluetun with PIA + WireGuard.\nWhy PIA?\nPort forwarding support (Mullvad and Surfshark do not, reliably) Stable IP handoff Easy config docker exec -it gluetun curl ifconfig.me # → Confirms VPN IP The torrent containers (qBittorrent, FlareSolverr, Prowlarr) all run inside Gluetun’s network namespace, meaning they literally cannot reach the internet unless the VPN tunnel is healthy.\nHealthchecks Containers like Gluetun have built-in healthchecks Others (e.g. Plex, Sonarr) are monitored externally by Uptime Kuma Restart policies are set to unless-stopped Cronjobs handle watchdog behavior for mover scripts and backups Sanity Checks After Deployment Check Command Docker status docker ps -a Disk I/O iotop, df -h, lsblk VPN leaks docker exec gluetun curl ifconfig.me Plex metadata health Verified via web UI + Tautulli Volume mounts `mount Firewall scope sudo ufw status (inside VM) Lessons Learned Good Decisions Problems Encountered Isolating containers per directory NTFS-mounted folders caused UID/permission issues Using Gluetun with qBittorrent VPN took longer to start — needed depends_on and sleep loop Moving all configs to /mnt/ssd-linux Accidentally ran a test compose from /root and duplicated configs Using network_mode: host for Plex Broke Kuma monitor until I fixed the port mapping logic Status After Phase 3 Phase 3 turned a VM into a self-sustaining, observable stack of services. Everything now runs in Docker, isolated, and configured to survive restarts and minor failures without intervention.\nPhase 4: The Network Now that the Docker stack was humming along, it was time to address my nemesis.\nNetworking is where most people go overboard — VLANs, dual-NIC bonding, multi-subnet segmentation, and a firewall that requires a PhD. That’s not this setup. I wanted secure, observable, and boring. Boring is good.\nGoals for Networking LAN access to all services without port collisions Remote access (secure) for Plex, Plexamp, and Minecraft VPN tunneling for torrenting only — not the whole VM No double-NAT, no exposed admin panels Monitoring and healthchecks from any device, anywhere Baseline Network Layout [Internet]\r│\r[ISP Router]\r├── 192.168.0.X → Proxmox (homelab)\r└── 192.168.0.X → Debian VM (docker-host)\rDHCP reservations for both Proxmox and the Docker VM No static IPs hardcoded in OS — all managed by router bindings Network mode for containers: mixed (host for Plex, bridge for most others) Tailscale: The Remote Access Backbone Tailscale is a zero-config WireGuard overlay that just works.\nWhy Tailscale? No port forwarding required Works behind NAT and CGNAT Every device gets a stable private IP Supports MagicDNS for easier SSH (proxmox.ts.net) Share access securely to friends or devices (ACLs) tailscale up --authkey tskey-... tailscale status This replaced the need for OpenVPN or exposing sensitive ports to the public web.\nDDNS via DuckDNS For services I did want externally exposed (Plex, Minecraft), I set up a free DuckDNS subdomain:\nDomain: raw-tech.duckdns.org Token stored in .env file for the container Updates automatically on IP change via the DuckDNS container Nginx Proxy Manager Some services needed SSL and custom hostnames, but I didn’t want to maintain Nginx by hand. NPM provided:\nGUI-based reverse proxy setup LetsEncrypt SSL certs with auto-renew Granular access control Simple subdomain routing to Docker services Used only for:\nPlex (HTTPS via Cloudflare DNS validation) Minecraft Everything else remains LAN-only behind Tailscale.\nUFW Firewall Rules Firewall is enforced inside the Debian VM using UFW.\nNetwork Monitoring Uptime Kuma monitors external endpoints (DuckDNS) qBittorrent Gluetun container pings ifconfig.me to validate VPN IP Discord alerts if the VPN IP ever goes down Kuma dashboard includes TCP port monitors for Plex, Minecraft, SSH Lessons from Phase 4 Service Could’ve Been Better Tailscale for remote access Took trial-and-error to route Plex properly Status After Phase 4 Plex, Minecraft reachable externally over HTTPS All other services only accessible via LAN or Tailscale Firewall restricts all unexpected inbound traffic Remote access works without port-forwarding hell VPN tunnel validates torrent container IP on boot Nothing is open to the public that shouldn’t be Phase 5: The Monitoring “I’m always watching, Wazowski.”\nI wanted eyes on everything: uptime, disk space, container health, and remote access status.\nMonitoring Goals Track availability of every container (and the VM itself) Detect VPN leaks or tunnel failures Get alerts if Plex, Sonarr, or automation tools go offline Keep historical logs for debugging Homepage Homepage acts as a simple self-hosted dashboard to:\nDisplay current container statuses Link to all local services Show system info (disk, uptime, memory) I did try Heimdall but I prefer Homepage’s YAML-based config Mounted at /mnt/ssd-linux/docker/homepage/, the config is stored in:\n/mnt/ssd-linux/docker/homepage/\r├── config/\r│ ├── services.yaml\r│ ├── bookmarks.yaml\r│ ├── settings.yaml\r│ └── widgets.yaml\rAccessible at http://192.168.0.X:3005\nUptime Kuma Uptime Kuma tracks container availability via:\nTCP port probes HTTP endpoint checks API integrations (e.g. Plex, Sonarr, Radarr) External DuckDNS URLs (to validate remote exposure) Accessible at http://192.168.0.X:3001\nKuma database is stored at:\n/mnt/ssd-linux/docker/uptime-kuma/kuma.db Monitored Services Service Type Target Plex HTTP /status/sessions?X-Plex-Token Sonarr API (JSON) /api/system/status Radarr API (JSON) /api/system/status qBittorrent TCP Port 8082 Minecraft TCP Port 25565 Filebrowser HTTP Port 8085 Tailscale VPN Shell curl ifconfig.me via Gluetun SSH TCP Port 22 Discord Notifications Both Uptime Kuma, Plex watch status (Tautulli) and custom cronjobs send alerts via Discord webhook.\nSample mover script alert:\ncurl -X POST -H \"Content-Type: application/json\" -d '{\"content\":\"✅ Plex mover ran. 12 files moved.\"}' $DISCORD_WEBHOOK Kuma also uses the same webhook to send alerts for downed or restored services.\nLog Management All script logs are dumped to /mnt/ssd-linux/logs/ Mover script log: plex-mover.log Minecraft backup log: minecraft-backup.log Logrotate compresses old logs daily and rotates after 7 days Sample crontab entry:\n0 * * * * /mnt/ssd-linux/scripts/plex-mover.sh \u003e\u003e /mnt/ssd-linux/logs/plex-mover.log 2\u003e\u00261 Lessons from Phase 5 Win Problem Solved Using Kuma instead of Grafana stack Avoided overengineering External uptime for DuckDNS URLs Detected DNS propagation issues early Discord webhooks Instant visibility on automation status Homepage YAML config Easy to update, no GUI needed Status After Phase 5 Every critical service monitored for uptime Discord alerts sent on all failures or mover script completions Web dashboards live at 3001 (Kuma) and 3005 (Homepage) VPN IP validated and verified on boot Historical logs available for analysis Phase 6: The Audit “You either die a hero, or live long enough to run your own compliance check.”\nAt this point, everything worked. But working isn’t the same as being reliable. That’s where the audit came in. A full walkthrough of every component, permission, port, and config.\nThis wasn’t just about validating what I had built. It was about documenting it so Future Me, or anyone else, could recover, debug, or upgrade the system without starting from scratch.\nAudit Objectives Ensure all mount points persist across reboots Confirm correct container state, health, and restart behavior Validate firewall rules and VPN isolation Test remote access and DNS resolution Backup critical configs and volumes Generate a readable, re-runnable DRP (Disaster Recovery Plan) Filesystem \u0026 Mounts All drives are mounted via UUID using /etc/fstab.\nUUID=xxxxxx /mnt/plex-media ntfs-3g defaults 0 0 UUID=yyyyyy /mnt/ssd-linux ext4 defaults 0 2 UUID=zzzzzz /mnt/hdd ntfs-3g defaults 0 0 Checked using:\nlsblk mount | grep /mnt df -h Docker Healthcheck docker ps -a docker compose ps docker ps --filter 'status=restarting' Confirmed:\nNo containers stuck in restart loops Plex, Sonarr, Radarr, Tautulli reachable on LAN Config directories are writable and not root-owned Gluetun VPN container routes traffic securely VPN + Network Verification Verified Gluetun’s tunnel IP:\ndocker exec gluetun curl ifconfig.me Also verified:\nPlex remote access works over Tailscale Minecraft is externally reachable (via DuckDNS + port 25565) Uptime Kuma shows correct up/down history for every container No admin panels exposed to WAN Firewall Rules (UFW) Checked with:\nsudo ufw status numbered Confirmed:\nOnly required ports are exposed externally All other ports limited to LAN or Tailscale subnet Backup Strategy Plex metadata stored under /mnt/ssd-linux/plex/ Minecraft world backed up to tar.gz weekly Kuma DB backed up manually and versioned Docker volume configs backed up to /mnt/hdd/docker-backup/ using rsync rsync -avh /mnt/ssd-linux/ /mnt/hdd/docker-backup/ Disaster Recovery Procedures Proxmox snapshot or fresh install Restore VM or rebuild from clean ISO Reattach drives, verify mount points Reinstall Docker and Compose cd ~/docker \u0026\u0026 docker compose up -d Validate Plex, automation tools, VPN, Kuma Restore Minecraft Resume cronjobs and verify Discord alerts Lessons from Phase 6 Good Habits Pain Avoided fstab + UUIDs No boot hangs or missing drives Documented firewall rules Easy auditing + validation Verified VPN tunnel at boot No torrent leaks rsync for Docker configs Instant container recovery Status After Phase 6 Infrastructure fully audited and documented Mounts and configs persist after full shutdown Backups exist and are restorable Containers and VPN behave as expected Nothing is exposed that shouldn’t be Disaster recovery validated on paper and in practice The stack is now reliable, recoverable, and repeatable.\nProject complete.\nFinal Thoughts What started as a way to escape the chaos of modern streaming became a deep dive into infrastructure, automation, and self-reliance. Plex was just the spark. The real fire came from building the ecosystem, the orchestration, the failure handling, the why behind the how.\nI’d written plenty of PowerShell before, mostly quick automations, task schedulers, file movers. But Linux is a different world. It demands precision. It punishes sloppiness. But it rewards patience, curiosity, and discipline. This was my first proper foray into that world.\nIt’s also personal. I don’t want my media held hostage by cloud outages or licensing changes. I want a system that runs on my terms. No ads. No logins. No “now available with premium.” Just content I care about, delivered how and when I want it.\nI didn’t build this homelab out of necessity. I built it because I was tired of friction. Every script solved an annoyance. Every cronjob eliminated a chore. What began as a one-off Plex box became a platform, resilient, monitored, automated.\nThe wiring is messy. The screw’s still stripped. But the system works. It monitors itself. It alerts me. It’s documented. It’s fast. And above all: This one’s mine.\n",
  "wordCount" : "3832",
  "inLanguage": "en",
  "image":"http://localhost:1313/images/homelab/homelab.png","datePublished": "2025-05-17T00:00:00Z",
  "dateModified": "2025-05-17T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Ryan Witts"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/posts/homelab/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "RAW-TECH",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="RAW-TECH (Alt + H)">RAW-TECH</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/recipes/" title="Cookbook">
                    <span>Cookbook</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      The Homelab Project
    </h1>
    <div class="post-meta"><span title='2025-05-17 00:00:00 +0000 UTC'>May 17, 2025</span>&nbsp;·&nbsp;18 min&nbsp;·&nbsp;3832 words&nbsp;·&nbsp;Ryan Witts

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#why-i-built-this-homelab" aria-label="Why I Built This Homelab">Why I Built This Homelab</a><ul>
                        <ul>
                        
                <li>
                    <a href="#evolution-of-the-homelab" aria-label="Evolution of the Homelab">Evolution of the Homelab</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#phase-0-the-plan" aria-label="Phase 0: The Plan">Phase 0: The Plan</a><ul>
                        <ul>
                        
                <li>
                    <a href="#phase-breakdown" aria-label="Phase Breakdown">Phase Breakdown</a></li>
                <li>
                    <a href="#guiding-principles" aria-label="Guiding Principles">Guiding Principles</a></li>
                <li>
                    <a href="#core-service-list" aria-label="Core Service List">Core Service List</a></li>
                <li>
                    <a href="#storage-strategy" aria-label="Storage Strategy">Storage Strategy</a></li>
                <li>
                    <a href="#success-criteria" aria-label="Success Criteria">Success Criteria</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#phase-1-the-hardware" aria-label="Phase 1: The Hardware">Phase 1: The Hardware</a><ul>
                        <ul>
                        
                <li>
                    <a href="#base-system-hp-prodesk-400-g6-mini" aria-label="Base System: HP ProDesk 400 G6 Mini">Base System: HP ProDesk 400 G6 Mini</a></li>
                <li>
                    <a href="#hardware-spec--upgrade-plan" aria-label="Hardware Spec &amp; Upgrade Plan">Hardware Spec &amp; Upgrade Plan</a></li>
                <li>
                    <a href="#the-screw-incident" aria-label="The Screw Incident">The Screw Incident</a></li>
                <li>
                    <a href="#revised-strategy" aria-label="Revised Strategy">Revised Strategy</a></li>
                <li>
                    <a href="#cooling-considerations" aria-label="Cooling Considerations">Cooling Considerations</a></li>
                <li>
                    <a href="#bios-tweaks" aria-label="BIOS Tweaks">BIOS Tweaks</a></li>
                <li>
                    <a href="#what-worked--what-didnt" aria-label="What Worked / What Didn&rsquo;t">What Worked / What Didn&rsquo;t</a></li>
                <li>
                    <a href="#lessons-from-phase-1" aria-label="Lessons from Phase 1">Lessons from Phase 1</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#phase-2-the-operating-system" aria-label="Phase 2: The Operating System">Phase 2: The Operating System</a><ul>
                        <ul>
                        
                <li>
                    <a href="#why-proxmox" aria-label="Why Proxmox?">Why Proxmox?</a></li>
                <li>
                    <a href="#installation-checklist" aria-label="Installation Checklist">Installation Checklist</a></li>
                <li>
                    <a href="#proxmox-configuration-recap" aria-label="Proxmox Configuration Recap">Proxmox Configuration Recap</a></li>
                <li>
                    <a href="#vm-planning" aria-label="VM Planning">VM Planning</a></li>
                <li>
                    <a href="#minimal-debian-install-notes" aria-label="Minimal Debian Install Notes">Minimal Debian Install Notes</a><ul>
                        
                <li>
                    <a href="#installed-only" aria-label="Installed only:">Installed only:</a></li></ul>
                </li>
                <li>
                    <a href="#drive-mounts-via-fstab" aria-label="Drive Mounts via fstab">Drive Mounts via fstab</a></li>
                <li>
                    <a href="#sanity-checks-before-moving-on" aria-label="Sanity Checks Before Moving On">Sanity Checks Before Moving On</a></li>
                <li>
                    <a href="#observations-from-phase-2" aria-label="Observations from Phase 2">Observations from Phase 2</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#phase-3-the-docker--containers" aria-label="Phase 3: The Docker &amp; Containers">Phase 3: The Docker &amp; Containers</a><ul>
                        <ul>
                        
                <li>
                    <a href="#why-docker" aria-label="Why Docker?">Why Docker?</a></li>
                <li>
                    <a href="#folder-structure" aria-label="Folder Structure">Folder Structure</a></li>
                <li>
                    <a href="#core-stack-docker-compose-services" aria-label="Core Stack: Docker Compose Services">Core Stack: Docker Compose Services</a></li>
                <li>
                    <a href="#services-deployed" aria-label="Services Deployed">Services Deployed</a></li>
                <li>
                    <a href="#vpn-enforcement-gluetun" aria-label="VPN Enforcement (Gluetun)">VPN Enforcement (Gluetun)</a></li>
                <li>
                    <a href="#healthchecks" aria-label="Healthchecks">Healthchecks</a></li>
                <li>
                    <a href="#sanity-checks-after-deployment" aria-label="Sanity Checks After Deployment">Sanity Checks After Deployment</a></li>
                <li>
                    <a href="#lessons-learned" aria-label="Lessons Learned">Lessons Learned</a></li>
                <li>
                    <a href="#status-after-phase-3" aria-label="Status After Phase 3">Status After Phase 3</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#phase-4-the-network" aria-label="Phase 4: The Network">Phase 4: The Network</a><ul>
                        
                <li>
                    <a href="#goals-for-networking" aria-label="Goals for Networking">Goals for Networking</a></li>
                <li>
                    <a href="#baseline-network-layout" aria-label="Baseline Network Layout">Baseline Network Layout</a></li>
                <li>
                    <a href="#tailscale-the-remote-access-backbone" aria-label="Tailscale: The Remote Access Backbone">Tailscale: The Remote Access Backbone</a><ul>
                        
                <li>
                    <a href="#why-tailscale" aria-label="Why Tailscale?">Why Tailscale?</a></li></ul>
                </li>
                <li>
                    <a href="#ddns-via-duckdns" aria-label="DDNS via DuckDNS">DDNS via DuckDNS</a></li>
                <li>
                    <a href="#nginx-proxy-manager" aria-label="Nginx Proxy Manager">Nginx Proxy Manager</a></li>
                <li>
                    <a href="#ufw-firewall-rules" aria-label="UFW Firewall Rules">UFW Firewall Rules</a></li>
                <li>
                    <a href="#network-monitoring" aria-label="Network Monitoring">Network Monitoring</a></li>
                <li>
                    <a href="#lessons-from-phase-4" aria-label="Lessons from Phase 4">Lessons from Phase 4</a></li>
                <li>
                    <a href="#status-after-phase-4" aria-label="Status After Phase 4">Status After Phase 4</a></li></ul>
                </li>
                <li>
                    <a href="#phase-5-the-monitoring" aria-label="Phase 5: The Monitoring">Phase 5: The Monitoring</a><ul>
                        
                <li>
                    <a href="#monitoring-goals" aria-label="Monitoring Goals">Monitoring Goals</a></li>
                <li>
                    <a href="#homepage" aria-label="Homepage">Homepage</a></li>
                <li>
                    <a href="#uptime-kuma" aria-label="Uptime Kuma">Uptime Kuma</a></li>
                <li>
                    <a href="#monitored-services" aria-label="Monitored Services">Monitored Services</a></li>
                <li>
                    <a href="#discord-notifications" aria-label="Discord Notifications">Discord Notifications</a></li>
                <li>
                    <a href="#log-management" aria-label="Log Management">Log Management</a></li>
                <li>
                    <a href="#lessons-from-phase-5" aria-label="Lessons from Phase 5">Lessons from Phase 5</a></li>
                <li>
                    <a href="#status-after-phase-5" aria-label="Status After Phase 5">Status After Phase 5</a></li></ul>
                </li>
                <li>
                    <a href="#phase-6-the-audit" aria-label="Phase 6: The Audit">Phase 6: The Audit</a><ul>
                        
                <li>
                    <a href="#audit-objectives" aria-label="Audit Objectives">Audit Objectives</a></li>
                <li>
                    <a href="#filesystem--mounts" aria-label="Filesystem &amp; Mounts">Filesystem &amp; Mounts</a></li>
                <li>
                    <a href="#docker-healthcheck" aria-label="Docker Healthcheck">Docker Healthcheck</a></li>
                <li>
                    <a href="#vpn--network-verification" aria-label="VPN &#43; Network Verification">VPN + Network Verification</a></li>
                <li>
                    <a href="#firewall-rules-ufw" aria-label="Firewall Rules (UFW)">Firewall Rules (UFW)</a></li>
                <li>
                    <a href="#backup-strategy" aria-label="Backup Strategy">Backup Strategy</a></li>
                <li>
                    <a href="#disaster-recovery-procedures" aria-label="Disaster Recovery Procedures">Disaster Recovery Procedures</a></li>
                <li>
                    <a href="#lessons-from-phase-6" aria-label="Lessons from Phase 6">Lessons from Phase 6</a></li>
                <li>
                    <a href="#status-after-phase-6" aria-label="Status After Phase 6">Status After Phase 6</a></li></ul>
                </li>
                <li>
                    <a href="#final-thoughts" aria-label="Final Thoughts">Final Thoughts</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="why-i-built-this-homelab">Why I Built This Homelab<a hidden class="anchor" aria-hidden="true" href="#why-i-built-this-homelab">#</a></h1>
<p>Modern streaming culture is exhausting. Content is fragmented across too many services, each demanding money for one or two shows or films people actually care about. It’s frustrating to pay for access only to be served bloated catalogs, Marvel filler arcs, or algorithm-bait reality TV.</p>
<p>Self-hosted media libraries have emerged as a response to that, a way to curate a meaningful, ad-free experience. They’re often built around content that’s hard to access legally: older titles, niche genres, or region-locked releases. For many, setting up something like Plex is less about cost-saving and more about taking control.</p>
<blockquote>
<p><strong>Media self-hosting can be a form of protest.</strong></p></blockquote>
<p>It pushes back against a corporate model that prioritises subscriptions and ad revenue over storytelling. When content resonates, real fans often support it directly, through merchandise, physical media, or word of mouth, not passive monthly payments.</p>
<p>That’s why I started building my own infrastructure. It began as a simple setup, Plex on my PC with a few folders, but quickly evolved. I wanted <strong>automation</strong>, then <strong>Docker</strong>, then <strong>Linux</strong>, then <strong>monitoring</strong>, then a <strong>Minecraft server</strong> for friends.</p>
<blockquote>
<p><strong>Now, it&rsquo;s a full-blown homelab. Virtualised, containerised and automated.</strong></p></blockquote>
<hr>
<h3 id="evolution-of-the-homelab">Evolution of the Homelab<a hidden class="anchor" aria-hidden="true" href="#evolution-of-the-homelab">#</a></h3>
<table>
  <thead>
      <tr>
          <th>Stage</th>
          <th>Details</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Seed</td>
          <td>Plex on Windows. Manual file adds. Painful.</td>
      </tr>
      <tr>
          <td>Sprout</td>
          <td>Plex + Sonarr + Radarr on Windows. Messy file paths.</td>
      </tr>
      <tr>
          <td>Sapling</td>
          <td>Moved to Proxmox VM with Debian 12. Full CLI control.</td>
      </tr>
      <tr>
          <td>Tree</td>
          <td>Docker stack: VPN-tunneled qBittorrent, smart automation, live dashboard, Discord alerts.</td>
      </tr>
  </tbody>
</table>
<blockquote>
<p><strong>Each stage taught me valuable lessons, refined my approach, and ultimately resulted in the reliable, fully automated homelab.</strong></p></blockquote>
<hr>
<h1 id="phase-0-the-plan">Phase 0: The Plan<a hidden class="anchor" aria-hidden="true" href="#phase-0-the-plan">#</a></h1>
<p><small><em>&ldquo;I&rsquo;m gonna make a homelab you can&rsquo;t refuse.&rdquo;</em></small></p>
<p>Before touching a single cable or burning an ISO, I sat down and mapped out what I actually wanted from this project. Not just “install Plex,” but an intentional, modular stack that could evolve over time without collapsing under its own weight.</p>
<p>So I split the entire journey into phases, each with a clear goal, checklist, and rollback plan if things went sideways:</p>
<hr>
<h3 id="phase-breakdown">Phase Breakdown<a hidden class="anchor" aria-hidden="true" href="#phase-breakdown">#</a></h3>
<table>
  <thead>
      <tr>
          <th>Phase</th>
          <th>Goal</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Phase 0: The Plan</strong></td>
          <td>Define requirements, map out architecture, break the build into logical stages</td>
      </tr>
      <tr>
          <td><strong>Phase 1: The Hardware</strong></td>
          <td>Physically assemble and verify the server; install/upgrade components and prep BIOS</td>
      </tr>
      <tr>
          <td><strong>Phase 2: The Operating System</strong></td>
          <td>Install Proxmox VE and configure the VM that’ll host all containers</td>
      </tr>
      <tr>
          <td><strong>Phase 3: The Docker &amp; Containers</strong></td>
          <td>Build the entire stack — Plex, torrenting, automation, and more</td>
      </tr>
      <tr>
          <td><strong>Phase 4: The Network</strong></td>
          <td>Harden the network, configure VPNs, firewalls, subnets, and remote access</td>
      </tr>
      <tr>
          <td><strong>Phase 5: The Monitoring</strong></td>
          <td>Add Uptime Kuma, dashboards, alerts, logging, and uptime recovery tools</td>
      </tr>
      <tr>
          <td><strong>Phase 6: The Audit</strong></td>
          <td>Final checklist, cleanup, disaster recovery documentation, and postmortem</td>
      </tr>
  </tbody>
</table>
<hr>
<h3 id="guiding-principles">Guiding Principles<a hidden class="anchor" aria-hidden="true" href="#guiding-principles">#</a></h3>
<p>These were the rules I forced myself to follow at every step:</p>
<ul>
<li><strong>Self-contained</strong>: No reliance on third-party clouds. My server, my data.</li>
<li><strong>Recoverable</strong>: Snapshot, and document recovery paths for every service.</li>
<li><strong>Modular</strong>: Everything must run in Docker with isolated volumes, configs, and environment files.</li>
<li><strong>Secure</strong>: No exposed ports unless explicitly needed. VPN tunnel preferred. Minimal surface area.</li>
<li><strong>Documented</strong>: If it’s not written down, it doesn’t exist. Markdown logs or it didn’t happen.</li>
<li><strong>Observable</strong>: If something breaks, ensure it&rsquo;s visible</li>
</ul>
<hr>
<h3 id="core-service-list">Core Service List<a hidden class="anchor" aria-hidden="true" href="#core-service-list">#</a></h3>
<p>Here’s the minimum I scoped out for v1 of the homelab:</p>
<ul>
<li><strong>Plex</strong>: The heart of the project</li>
<li><strong>qBittorrent + Gluetun</strong>: Torrents tunneled through VPN</li>
<li><strong>Sonarr / Radarr / Prowlarr</strong>: Automated content acquisition</li>
<li><strong>Tautulli</strong>: Usage tracking and watched status</li>
<li><strong>Uptime Kuma</strong>: Monitor critical services</li>
<li><strong>Homepage</strong>: Self-hosted status/dashboard</li>
<li><strong>Nginx Proxy Manager</strong>: SSL and reverse proxy</li>
<li><strong>Minecraft</strong>: Because why not</li>
<li><strong>FileBrowser</strong>: Lightweight UI for browsing storage</li>
<li><strong>DuckDNS</strong>: Basic dynamic DNS for remote access</li>
<li><strong>Tailscale</strong>: Secure overlay VPN across devices</li>
</ul>
<hr>
<h3 id="storage-strategy">Storage Strategy<a hidden class="anchor" aria-hidden="true" href="#storage-strategy">#</a></h3>
<p>This was critical. I wasn’t going to throw media and config files on the same disk like a scrub.</p>
<table>
  <thead>
      <tr>
          <th>Drive</th>
          <th>Mount Path</th>
          <th>Purpose</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>120GB SSD</strong></td>
          <td>Proxmox OS</td>
          <td>Host the hypervisor</td>
      </tr>
      <tr>
          <td><strong>1TB SSD (EXT4)</strong></td>
          <td><code>/mnt/ssd-linux</code></td>
          <td>All Docker configs, volumes, metadata</td>
      </tr>
      <tr>
          <td><strong>2TB SSD (NTFS)</strong></td>
          <td><code>/mnt/plex-media</code></td>
          <td>Hot storage for recently added media</td>
      </tr>
      <tr>
          <td><strong>8TB HDD (NTFS)</strong></td>
          <td><code>/mnt/hdd</code></td>
          <td>Cold archive for watched or backup content</td>
      </tr>
  </tbody>
</table>
<p>Each one passed through cleanly to the Debian VM using <code>scsiX</code> raw device passthrough in Proxmox. <code>ntfs-3g</code> handles mount duties for the NTFS partitions in <code>/etc/fstab</code>.</p>
<hr>
<h3 id="success-criteria">Success Criteria<a hidden class="anchor" aria-hidden="true" href="#success-criteria">#</a></h3>
<p>How I knew this project would be “done” (for now):</p>
<ul>
<li>Plex serves content instantly on LAN and remote (via Tailscale)</li>
<li>Automation pulls, renames, and organises new content without manual input</li>
<li>Services restart automatically after crash or reboot</li>
<li>Monitoring alerts me if anything breaks — including the mover script</li>
<li>Minecraft server is online, backed up, and doesn’t turn into a laggy mess</li>
<li>Full disaster recovery is possible with only access to my external drives and Markdown docs</li>
</ul>
<blockquote>
<p>That was the blueprint.</p></blockquote>
<hr>
<h1 id="phase-1-the-hardware">Phase 1: The Hardware<a hidden class="anchor" aria-hidden="true" href="#phase-1-the-hardware">#</a></h1>
<p><small><em>&ldquo;Say hello to my little friend&hellip; the ProDesk.&rdquo;</em></small></p>
<p>This phase was supposed to be simple: unbox, upgrade, install. In reality? It turned into a low-level boss fight with HP’s factory screws.</p>
<hr>
<h3 id="base-system-hp-prodesk-400-g6-mini">Base System: HP ProDesk 400 G6 Mini<a hidden class="anchor" aria-hidden="true" href="#base-system-hp-prodesk-400-g6-mini">#</a></h3>
<p>I chose this machine for a few reasons:</p>
<ul>
<li><strong>Small form factor</strong>: Fits anywhere, whisper quiet</li>
<li><strong>Low power draw</strong>: Ideal for 24/7 uptime</li>
<li><strong>Surprisingly capable</strong>: 6-core, 12-thread CPU and 64GB RAM capacity</li>
<li><strong>Cheap second-hand</strong>: Plenty available from ex-corporate stock</li>
</ul>
<hr>
<h3 id="hardware-spec--upgrade-plan">Hardware Spec &amp; Upgrade Plan<a hidden class="anchor" aria-hidden="true" href="#hardware-spec--upgrade-plan">#</a></h3>
<p>I wanted a future-proof homelab node that could run Proxmox and host multiple VMs + containers without breaking a sweat.</p>
<table>
  <thead>
      <tr>
          <th>Component</th>
          <th>Model / Detail</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>CPU</strong></td>
          <td>Intel i5-10500T (6C/12T @ 2.3GHz)</td>
      </tr>
      <tr>
          <td><strong>RAM</strong></td>
          <td>64GB DDR4 SODIMM (2×32GB Crucial 3200MHz CL22)</td>
      </tr>
      <tr>
          <td><strong>Primary Disk</strong></td>
          <td>1TB Kingston NV2 Gen4 NVMe SSD</td>
      </tr>
      <tr>
          <td><strong>Backup Boot</strong></td>
          <td>120GB Generic M.2 SSD (fallback boot volume)</td>
      </tr>
      <tr>
          <td><strong>External #1</strong></td>
          <td>2TB WD SN770 (hot media) via USB-C</td>
      </tr>
      <tr>
          <td><strong>External #2</strong></td>
          <td>8TB Seagate Expansion HDD (archive) via USB-A</td>
      </tr>
      <tr>
          <td><strong>Enclosure</strong></td>
          <td>USB-C NVMe enclosure (used for the 1TB &amp; 2TB)</td>
      </tr>
  </tbody>
</table>
<p>Everything went smoothly — until I tried to swap the internal SSD.</p>
<hr>
<h3 id="the-screw-incident">The Screw Incident<a hidden class="anchor" aria-hidden="true" href="#the-screw-incident">#</a></h3>
<p>The pre-installed 120GB M.2 SSD was locked in place with a <strong>factory-stripped screw</strong>. HP must’ve torqued it with the force of a thousand suns. I tried everything:</p>
<ul>
<li>Rubber bands</li>
<li>Precision extraction bits</li>
<li>Superglue (desperation move)</li>
<li>Swearing profusely</li>
</ul>
<blockquote>
<p>Nothing worked. That screw wasn’t coming out without tearing the board.</p></blockquote>
<p>At this point, I had two options:</p>
<ol>
<li><strong>Destroy the 120GB SSD</strong> and force in the 1TB NVMe — risky and irreversible</li>
<li><strong>Leave the 120GB in place</strong> and install the 1TB externally in a USB-C NVMe enclosure</li>
</ol>
<p>I chose option 2. It wasn’t ideal — but it worked. And it let me move forward without bricking the hardware on day one.</p>
<hr>
<h3 id="revised-strategy">Revised Strategy<a hidden class="anchor" aria-hidden="true" href="#revised-strategy">#</a></h3>
<p>Instead of running Proxmox off the fast 1TB NVMe, I flipped the plan:</p>
<table>
  <thead>
      <tr>
          <th>Device</th>
          <th>Role</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>120GB SSD</strong></td>
          <td>Host Proxmox OS (internal, bootable)</td>
      </tr>
      <tr>
          <td><strong>1TB SSD (EXT4)</strong></td>
          <td>Docker volumes, configs, Plex metadata</td>
      </tr>
      <tr>
          <td><strong>2TB SSD (NTFS)</strong></td>
          <td>mount for unwatched media</td>
      </tr>
      <tr>
          <td><strong>8TB HDD (NTFS)</strong></td>
          <td>Cold storage for watched media / backups</td>
      </tr>
  </tbody>
</table>
<p>All external drives passed through to the <code>docker-host</code> VM using <strong>Proxmox raw device passthrough</strong> (<code>scsiX: /dev/sdX,format=raw</code>). This gave the VM native block-level access to each disk.</p>
<hr>
<h3 id="cooling-considerations">Cooling Considerations<a hidden class="anchor" aria-hidden="true" href="#cooling-considerations">#</a></h3>
<p>With an NVMe drive running full-time in a plastic USB-C enclosure, I added:</p>
<ul>
<li>Low-profile thermal pads inside the case</li>
</ul>
<blockquote>
<p>I don’t want my main Docker volume drive thermal throttling mid-transcode.</p></blockquote>
<hr>
<h3 id="bios-tweaks">BIOS Tweaks<a hidden class="anchor" aria-hidden="true" href="#bios-tweaks">#</a></h3>
<p>To prep for virtualization and passthrough, I adjusted:</p>
<ul>
<li>VT-x and VT-d: Enabled</li>
<li>Secure Boot: Disabled</li>
<li>Legacy Boot: Disabled</li>
<li>Boot Priority: Set to internal 120GB SSD</li>
</ul>
<hr>
<h3 id="what-worked--what-didnt">What Worked / What Didn&rsquo;t<a hidden class="anchor" aria-hidden="true" href="#what-worked--what-didnt">#</a></h3>
<table>
  <thead>
      <tr>
          <th>What Worked Well</th>
          <th>What Was a Nightmare</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Upgrading RAM to 64GB</td>
          <td>Swapping the internal SSD</td>
      </tr>
      <tr>
          <td>USB passthrough for Docker volumes</td>
          <td>Thermal throttling risk on NVMe over USB-C</td>
      </tr>
      <tr>
          <td>BIOS config for VT-x / VT-d</td>
          <td>HP’s anti-human screw design</td>
      </tr>
      <tr>
          <td>Modular drive setup with fstab</td>
          <td>NTFS quirks on cold storage drives</td>
      </tr>
  </tbody>
</table>
<hr>
<h3 id="lessons-from-phase-1">Lessons from Phase 1<a hidden class="anchor" aria-hidden="true" href="#lessons-from-phase-1">#</a></h3>
<ul>
<li>Always test drive mounts with <code>UUID</code> and <code>fstab</code> before assuming they&rsquo;ll survive a reboot</li>
<li>Avoid gluing anything inside your PC unless you hate yourself</li>
<li>64GB RAM might be overkill</li>
<li>USB-C NVMe is surprisingly fast (~900MB/s) but <strong>nowhere near</strong> native Gen4 speeds</li>
</ul>
<hr>
<p>With the hardware stable, drives mounted, and BIOS dialed in, I was finally ready to install the hypervisor and start spinning up VMs.</p>
<hr>
<h1 id="phase-2-the-operating-system">Phase 2: The Operating System<a hidden class="anchor" aria-hidden="true" href="#phase-2-the-operating-system">#</a></h1>
<p><small><em>&ldquo;Yer a wizard, Debian.&rdquo;</em></small></p>
<p>With the hardware finally stable (screw rage behind me), it was time to give this machine a brain.</p>
<p>There were many ways I could&rsquo;ve gone here; bare metal Docker, Windows Server, Unraid, TrueNAS&hellip; but I wanted:</p>
<ul>
<li><strong>Virtualisation support</strong> for running multiple isolated environments</li>
<li><strong>A strong CLI and community</strong></li>
<li><strong>Total control over storage, snapshots, and PCI passthrough</strong></li>
</ul>
<p>There was only one option that hit all the marks without being bloated: <strong>Proxmox VE</strong>.</p>
<hr>
<h3 id="why-proxmox">Why Proxmox?<a hidden class="anchor" aria-hidden="true" href="#why-proxmox">#</a></h3>
<ul>
<li>Free, open-source hypervisor</li>
<li>Clean web UI + full CLI access</li>
<li>Built-in support for raw disk passthrough</li>
<li>Powerful snapshot/backup tools</li>
<li>Debian-based</li>
</ul>
<hr>
<h3 id="installation-checklist">Installation Checklist<a hidden class="anchor" aria-hidden="true" href="#installation-checklist">#</a></h3>
<p><strong>OS:</strong> Proxmox VE 8.1<br>
<strong>Install Method:</strong> Flashed ISO via Rufus → Booted from USB<br>
<strong>Target Disk:</strong> Internal 120GB SSD<br>
<strong>Post-Install Tasks:</strong></p>
<ul>
<li>Set static IP to <code>192.168.0.x</code> (reserved via router DHCP)</li>
<li>Disable subscription nags (<code>sed</code> + APT patch)</li>
<li>Rename node to <code>homelab</code></li>
<li>Enable remote SSH access</li>
<li>Create non-root user for SSH</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Confirm node config</span>
</span></span><span style="display:flex;"><span>pveversion -v
</span></span></code></pre></div><hr>
<h3 id="proxmox-configuration-recap">Proxmox Configuration Recap<a hidden class="anchor" aria-hidden="true" href="#proxmox-configuration-recap">#</a></h3>
<table>
  <thead>
      <tr>
          <th>Setting</th>
          <th>Value</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Hostname</td>
          <td><code>homelab</code></td>
      </tr>
      <tr>
          <td>Management IP</td>
          <td><code>192.168.0.X</code> (static)</td>
      </tr>
      <tr>
          <td>Web UI</td>
          <td><code>https://192.168.0.X:8006</code></td>
      </tr>
      <tr>
          <td>Subscription Popup</td>
          <td>Disabled</td>
      </tr>
  </tbody>
</table>
<hr>
<h3 id="vm-planning">VM Planning<a hidden class="anchor" aria-hidden="true" href="#vm-planning">#</a></h3>
<p>My philosophy here was &ldquo;one VM to rule them all&rdquo;, but only if that VM was solid. I didn’t want 15 micro-VMs doing niche tasks badly.</p>
<p>Primary VM: <code>docker-host</code></p>
<table>
  <thead>
      <tr>
          <th>Spec</th>
          <th>Value</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>OS</td>
          <td>Debian 12 (Bookworm, minimal install)</td>
      </tr>
      <tr>
          <td>vCPU</td>
          <td>8 cores</td>
      </tr>
      <tr>
          <td>RAM</td>
          <td>48GB</td>
      </tr>
      <tr>
          <td>Disk (internal)</td>
          <td>48GB on Proxmox SSD</td>
      </tr>
      <tr>
          <td>Mounts</td>
          <td>1TB EXT4 (<code>/mnt/ssd-linux</code>), 2TB NTFS (<code>/mnt/plex-media</code>), 8TB NTFS (<code>/mnt/hdd</code>)</td>
      </tr>
      <tr>
          <td>IP Address</td>
          <td><code>192.168.0.X</code> (static via DHCP)</td>
      </tr>
  </tbody>
</table>
<p><img alt="caption" loading="lazy" src="/images/homelab/proxmox_ui.png"></p>
<hr>
<h3 id="minimal-debian-install-notes">Minimal Debian Install Notes<a hidden class="anchor" aria-hidden="true" href="#minimal-debian-install-notes">#</a></h3>
<p>Debian was installed manually using the netinst ISO.</p>
<h4 id="installed-only">Installed only:<a hidden class="anchor" aria-hidden="true" href="#installed-only">#</a></h4>
<ul>
<li>OpenSSH server</li>
<li>sudo</li>
<li>curl, wget, git, htop, rsync</li>
<li>ntfs-3g for media drives</li>
<li>Docker CE + Docker Compose v2 (installed via apt repo)</li>
</ul>
<p>I avoided the Desktop Environment, Snap packages, or any auto-magic config. Keep it lean.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo apt update <span style="color:#f92672">&amp;&amp;</span> sudo apt upgrade -y
</span></span><span style="display:flex;"><span>sudo apt install docker.io docker-compose -y
</span></span></code></pre></div><hr>
<h3 id="drive-mounts-via-fstab">Drive Mounts via fstab<a hidden class="anchor" aria-hidden="true" href="#drive-mounts-via-fstab">#</a></h3>
<p>Mounts were UUID-based and cleanly defined for permanence across reboots.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># /etc/fstab entries (UUIDs redacted)</span>
</span></span><span style="display:flex;"><span>UUID<span style="color:#f92672">=</span>xxxxxx /mnt/plex-media ntfs-3g defaults <span style="color:#ae81ff">0</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>UUID<span style="color:#f92672">=</span>yyyyyy /mnt/ssd-linux ext4 defaults <span style="color:#ae81ff">0</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>UUID<span style="color:#f92672">=</span>zzzzzz /mnt/hdd ntfs-3g defaults <span style="color:#ae81ff">0</span> <span style="color:#ae81ff">0</span>
</span></span></code></pre></div><p>All drives were passed through from Proxmox using raw disk passthrough <code>(scsiX)</code> to give the VM full block access.</p>
<hr>
<h3 id="sanity-checks-before-moving-on">Sanity Checks Before Moving On<a hidden class="anchor" aria-hidden="true" href="#sanity-checks-before-moving-on">#</a></h3>
<ul>
<li>VM boots cleanly</li>
<li>SSH access stable (ssh XXXX@docker-host)</li>
<li>All 3 drives auto-mount and are writable</li>
<li>docker ps and docker compose run fine</li>
<li>Static IP reachable via LAN</li>
<li>fstab doesn’t hang at boot</li>
<li>/etc/hosts and /etc/hostname match</li>
<li>UFW firewall installed and tested</li>
</ul>
<hr>
<h3 id="observations-from-phase-2">Observations from Phase 2<a hidden class="anchor" aria-hidden="true" href="#observations-from-phase-2">#</a></h3>
<table>
  <thead>
      <tr>
          <th>Good Decisions</th>
          <th>Could&rsquo;ve Been Better</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Using Debian minimal install</td>
          <td>Should&rsquo;ve pre-written fstab entries to avoid a reboot cycle</td>
      </tr>
      <tr>
          <td>SSH from day 1</td>
          <td>Consider setting up earlier for convenience</td>
      </tr>
      <tr>
          <td>Single powerful VM</td>
          <td>Might need to split in future if resource contention appears</td>
      </tr>
      <tr>
          <td>Using EXT4 for Docker configs</td>
          <td>NTFS was <em>not</em> fun to script across</td>
      </tr>
  </tbody>
</table>
<hr>
<h1 id="phase-3-the-docker--containers">Phase 3: The Docker &amp; Containers<a hidden class="anchor" aria-hidden="true" href="#phase-3-the-docker--containers">#</a></h1>
<p><small><em>&ldquo;Dock Hard&rdquo;</em></small></p>
<p>With Debian installed and the drives mounted, it was finally time to start building the real infrastructure. This is where the homelab went from “just a VM” to a full service stack.</p>
<hr>
<h3 id="why-docker">Why Docker?<a hidden class="anchor" aria-hidden="true" href="#why-docker">#</a></h3>
<p>Because I’m not interested in:</p>
<ul>
<li>Manually setting up services</li>
<li>Dealing with conflicting dependencies</li>
<li>Running a script and praying it survives a reboot</li>
</ul>
<p>Docker gives me:</p>
<ul>
<li><strong>Isolation</strong>: Every service lives in its own box</li>
<li><strong>Portability</strong>: Move everything by copying one folder</li>
<li><strong>Repeatability</strong>: Rebuild my entire stack in seconds</li>
<li><strong>Observability</strong>: Every container has logs, healthchecks, and restart policies</li>
</ul>
<p>And most importantly: <strong>Docker Compose</strong> lets me define the entire stack in one YAML file.</p>
<hr>
<h3 id="folder-structure">Folder Structure<a hidden class="anchor" aria-hidden="true" href="#folder-structure">#</a></h3>
<p>All containers live under <code>/mnt/ssd-linux/docker</code>, which sits on the 1TB EXT4 SSD passed through to the VM. It&rsquo;s clean, fast, and not affected by NTFS permission weirdness.</p>
<pre><code>
/mnt/ssd-linux/
└── docker/
    ├── compose.yml
    ├── plex/
    ├── tautulli/
    ├── qbittorrent/
    ├── gluetun/
    ├── sonarr/
    ├── radarr/
    ├── prowlarr/
    ├── homepage/
    ├── uptime-kuma/
    ├── filebrowser/
    └── scripts/
</code></pre>
<hr>
<h3 id="core-stack-docker-compose-services">Core Stack: Docker Compose Services<a hidden class="anchor" aria-hidden="true" href="#core-stack-docker-compose-services">#</a></h3>
<p>Here’s a trimmed snapshot of what my <code>docker-compose.yml</code> looked like during this phase:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">version</span>: <span style="color:#e6db74">&#34;3.9&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">services</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">plex</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">image</span>: <span style="color:#ae81ff">lscr.io/linuxserver/plex</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">network_mode</span>: <span style="color:#ae81ff">host</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">volumes</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#ae81ff">./plex:/config</span>
</span></span><span style="display:flex;"><span>      - <span style="color:#ae81ff">/mnt/plex-media:/movies</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">environment</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#ae81ff">PUID=1000</span>
</span></span><span style="display:flex;"><span>      - <span style="color:#ae81ff">PGID=1000</span>
</span></span><span style="display:flex;"><span>      - <span style="color:#ae81ff">VERSION=docker</span>
</span></span><span style="display:flex;"><span>      - <span style="color:#ae81ff">TZ=Europe/London</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">restart</span>: <span style="color:#ae81ff">unless-stopped</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">gluetun</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">image</span>: <span style="color:#ae81ff">qmcgaw/gluetun</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">cap_add</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#ae81ff">NET_ADMIN</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">devices</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#ae81ff">/dev/net/tun</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">environment</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#ae81ff">VPN_SERVICE_PROVIDER=pia</span>
</span></span><span style="display:flex;"><span>      - <span style="color:#ae81ff">VPN_TYPE=wireguard</span>
</span></span><span style="display:flex;"><span>      - <span style="color:#ae81ff">SERVER_CITIES=Amsterdam</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">volumes</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#ae81ff">./gluetun:/gluetun</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">restart</span>: <span style="color:#ae81ff">unless-stopped</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">qbittorrent</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">image</span>: <span style="color:#ae81ff">linuxserver/qbittorrent</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">network_mode</span>: <span style="color:#e6db74">&#34;service:gluetun&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">depends_on</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#ae81ff">gluetun</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">environment</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#ae81ff">WEBUI_PORT=8082</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">volumes</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#ae81ff">./qbittorrent:/config</span>
</span></span><span style="display:flex;"><span>      - <span style="color:#ae81ff">/mnt/plex-media/downloads:/downloads</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">restart</span>: <span style="color:#ae81ff">unless-stopped</span>
</span></span></code></pre></div><hr>
<h3 id="services-deployed">Services Deployed<a hidden class="anchor" aria-hidden="true" href="#services-deployed">#</a></h3>
<table>
  <thead>
      <tr>
          <th>Service</th>
          <th>Port(s)</th>
          <th>Notes</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Plex</strong></td>
          <td>32400</td>
          <td>Core media server. Runs in <code>host</code> mode for LAN discovery &amp; DLNA.</td>
      </tr>
      <tr>
          <td><strong>qBittorrent</strong></td>
          <td>8082 (via VPN)</td>
          <td>Web UI tunneled through Gluetun (PIA WireGuard).</td>
      </tr>
      <tr>
          <td><strong>Gluetun</strong></td>
          <td>—</td>
          <td>VPN container (PIA, WireGuard). Tunnel for torrent + indexers.</td>
      </tr>
      <tr>
          <td><strong>Sonarr</strong></td>
          <td>8989</td>
          <td>TV automation. Behind Gluetun VPN.</td>
      </tr>
      <tr>
          <td><strong>Radarr</strong></td>
          <td>7878</td>
          <td>Movie automation. Behind Gluetun VPN.</td>
      </tr>
      <tr>
          <td><strong>Prowlarr</strong></td>
          <td>9696</td>
          <td>Indexer manager. Behind Gluetun VPN.</td>
      </tr>
      <tr>
          <td><strong>Tautulli</strong></td>
          <td>8181</td>
          <td>Plex stats and activity tracker.</td>
      </tr>
      <tr>
          <td><strong>Homepage</strong></td>
          <td>3005</td>
          <td>Custom homelab dashboard. Internal only.</td>
      </tr>
      <tr>
          <td><strong>Uptime Kuma</strong></td>
          <td>3001</td>
          <td>Status monitoring. Internal only.</td>
      </tr>
      <tr>
          <td><strong>FileBrowser</strong></td>
          <td>8085</td>
          <td>Lightweight file manager. Internal only.</td>
      </tr>
      <tr>
          <td><strong>DuckDNS</strong></td>
          <td>—</td>
          <td>Dynamic DNS. Updates external IP for domain access.</td>
      </tr>
      <tr>
          <td><strong>Minecraft</strong></td>
          <td>25565</td>
          <td>Hosted LAN game server. Publicly exposed</td>
      </tr>
      <tr>
          <td><strong>Nginx Proxy Manager</strong></td>
          <td>80 / 443</td>
          <td>GUI reverse proxy.</td>
      </tr>
      <tr>
          <td><strong>Tailscale</strong></td>
          <td>—</td>
          <td>Private overlay network for remote SSH/management.</td>
      </tr>
      <tr>
          <td><strong>FlareSolverr</strong></td>
          <td>8191</td>
          <td>Captcha solver for Prowlarr. Routed via Gluetun.</td>
      </tr>
      <tr>
          <td><strong>Netdata</strong></td>
          <td>19999</td>
          <td>Real-time system monitoring. Internal access only.</td>
      </tr>
      <tr>
          <td><strong>Portainer</strong></td>
          <td>9000</td>
          <td>Docker management GUI. Internal only.</td>
      </tr>
  </tbody>
</table>
<hr>
<h3 id="vpn-enforcement-gluetun">VPN Enforcement (Gluetun)<a hidden class="anchor" aria-hidden="true" href="#vpn-enforcement-gluetun">#</a></h3>
<p>Torrenting is routed entirely through a VPN using Gluetun with PIA + WireGuard.</p>
<p>Why PIA?</p>
<ul>
<li>Port forwarding support (Mullvad and Surfshark do not, reliably)</li>
<li>Stable IP handoff</li>
<li>Easy config</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker exec -it gluetun curl ifconfig.me
</span></span><span style="display:flex;"><span><span style="color:#75715e"># → Confirms VPN IP</span>
</span></span></code></pre></div><p>The torrent containers (qBittorrent, FlareSolverr, Prowlarr) all run inside Gluetun’s network namespace, meaning they literally cannot reach the internet unless the VPN tunnel is healthy.</p>
<hr>
<h3 id="healthchecks">Healthchecks<a hidden class="anchor" aria-hidden="true" href="#healthchecks">#</a></h3>
<ul>
<li>Containers like Gluetun have built-in healthchecks</li>
<li>Others (e.g. Plex, Sonarr) are monitored externally by Uptime Kuma</li>
<li>Restart policies are set to <code>unless-stopped</code></li>
<li>Cronjobs handle watchdog behavior for mover scripts and backups</li>
</ul>
<hr>
<h3 id="sanity-checks-after-deployment">Sanity Checks After Deployment<a hidden class="anchor" aria-hidden="true" href="#sanity-checks-after-deployment">#</a></h3>
<table>
  <thead>
      <tr>
          <th>Check</th>
          <th>Command</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Docker status</td>
          <td><code>docker ps -a</code></td>
      </tr>
      <tr>
          <td>Disk I/O</td>
          <td><code>iotop</code>, <code>df -h</code>, <code>lsblk</code></td>
      </tr>
      <tr>
          <td>VPN leaks</td>
          <td><code>docker exec gluetun curl ifconfig.me</code></td>
      </tr>
      <tr>
          <td>Plex metadata health</td>
          <td>Verified via web UI + Tautulli</td>
      </tr>
      <tr>
          <td>Volume mounts</td>
          <td>`mount</td>
      </tr>
      <tr>
          <td>Firewall scope</td>
          <td><code>sudo ufw status</code> (inside VM)</td>
      </tr>
  </tbody>
</table>
<hr>
<h3 id="lessons-learned">Lessons Learned<a hidden class="anchor" aria-hidden="true" href="#lessons-learned">#</a></h3>
<table>
  <thead>
      <tr>
          <th>Good Decisions</th>
          <th>Problems Encountered</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Isolating containers per directory</td>
          <td>NTFS-mounted folders caused UID/permission issues</td>
      </tr>
      <tr>
          <td>Using Gluetun with qBittorrent</td>
          <td>VPN took longer to start — needed <code>depends_on</code> and sleep loop</td>
      </tr>
      <tr>
          <td>Moving all configs to <code>/mnt/ssd-linux</code></td>
          <td>Accidentally ran a test compose from <code>/root</code> and duplicated configs</td>
      </tr>
      <tr>
          <td>Using <code>network_mode: host</code> for Plex</td>
          <td>Broke Kuma monitor until I fixed the port mapping logic</td>
      </tr>
  </tbody>
</table>
<hr>
<h3 id="status-after-phase-3">Status After Phase 3<a hidden class="anchor" aria-hidden="true" href="#status-after-phase-3">#</a></h3>
<p>Phase 3 turned a VM into a self-sustaining, observable stack of services. Everything now runs in Docker, isolated, and configured to survive restarts and minor failures without intervention.</p>
<hr>
<h1 id="phase-4-the-network">Phase 4: The Network<a hidden class="anchor" aria-hidden="true" href="#phase-4-the-network">#</a></h1>
<p>Now that the Docker stack was humming along, it was time to address my nemesis.</p>
<p>Networking is where most people go overboard — VLANs, dual-NIC bonding, multi-subnet segmentation, and a firewall that requires a PhD. That’s not this setup. I wanted secure, observable, and boring. Boring is good.</p>
<hr>
<h2 id="goals-for-networking">Goals for Networking<a hidden class="anchor" aria-hidden="true" href="#goals-for-networking">#</a></h2>
<ul>
<li>LAN access to all services without port collisions</li>
<li>Remote access (secure) for Plex, Plexamp, and Minecraft</li>
<li>VPN tunneling for torrenting only — not the whole VM</li>
<li>No double-NAT, no exposed admin panels</li>
<li>Monitoring and healthchecks from any device, anywhere</li>
</ul>
<hr>
<h2 id="baseline-network-layout">Baseline Network Layout<a hidden class="anchor" aria-hidden="true" href="#baseline-network-layout">#</a></h2>
<pre><code>
[Internet]
   │
[ISP Router]
   ├── 192.168.0.X → Proxmox (homelab)
   └── 192.168.0.X → Debian VM (docker-host)
</code></pre>
<ul>
<li>DHCP reservations for both Proxmox and the Docker VM</li>
<li>No static IPs hardcoded in OS — all managed by router bindings</li>
<li>Network mode for containers: mixed (<code>host</code> for Plex, bridge for most others)</li>
</ul>
<hr>
<h2 id="tailscale-the-remote-access-backbone">Tailscale: The Remote Access Backbone<a hidden class="anchor" aria-hidden="true" href="#tailscale-the-remote-access-backbone">#</a></h2>
<p>Tailscale is a zero-config WireGuard overlay that just works.</p>
<h3 id="why-tailscale">Why Tailscale?<a hidden class="anchor" aria-hidden="true" href="#why-tailscale">#</a></h3>
<ul>
<li>No port forwarding required</li>
<li>Works behind NAT and CGNAT</li>
<li>Every device gets a stable private IP</li>
<li>Supports MagicDNS for easier SSH (<code>proxmox.ts.net</code>)</li>
<li>Share access securely to friends or devices (ACLs)</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>tailscale up --authkey tskey-...
</span></span><span style="display:flex;"><span>tailscale status
</span></span></code></pre></div><p>This replaced the need for OpenVPN or exposing sensitive ports to the public web.</p>
<hr>
<h2 id="ddns-via-duckdns">DDNS via DuckDNS<a hidden class="anchor" aria-hidden="true" href="#ddns-via-duckdns">#</a></h2>
<p>For services I <em>did</em> want externally exposed (Plex, Minecraft), I set up a free DuckDNS subdomain:</p>
<ul>
<li>Domain: <code>raw-tech.duckdns.org</code></li>
<li>Token stored in <code>.env</code> file for the container</li>
<li>Updates automatically on IP change via the DuckDNS container</li>
</ul>
<hr>
<h2 id="nginx-proxy-manager">Nginx Proxy Manager<a hidden class="anchor" aria-hidden="true" href="#nginx-proxy-manager">#</a></h2>
<p>Some services needed SSL and custom hostnames, but I didn’t want to maintain Nginx by hand. NPM provided:</p>
<ul>
<li>GUI-based reverse proxy setup</li>
<li>LetsEncrypt SSL certs with auto-renew</li>
<li>Granular access control</li>
<li>Simple subdomain routing to Docker services</li>
</ul>
<p>Used only for:</p>
<ul>
<li>Plex (HTTPS via Cloudflare DNS validation)</li>
<li>Minecraft</li>
</ul>
<p>Everything else remains LAN-only behind Tailscale.</p>
<hr>
<h2 id="ufw-firewall-rules">UFW Firewall Rules<a hidden class="anchor" aria-hidden="true" href="#ufw-firewall-rules">#</a></h2>
<p>Firewall is enforced <em>inside</em> the Debian VM using UFW.</p>
<hr>
<h2 id="network-monitoring">Network Monitoring<a hidden class="anchor" aria-hidden="true" href="#network-monitoring">#</a></h2>
<ul>
<li>Uptime Kuma monitors external endpoints (DuckDNS)</li>
<li>qBittorrent Gluetun container pings <code>ifconfig.me</code> to validate VPN IP</li>
<li>Discord alerts if the VPN IP ever goes down</li>
<li>Kuma dashboard includes TCP port monitors for Plex, Minecraft, SSH</li>
</ul>
<hr>
<h2 id="lessons-from-phase-4">Lessons from Phase 4<a hidden class="anchor" aria-hidden="true" href="#lessons-from-phase-4">#</a></h2>
<table>
  <thead>
      <tr>
          <th>Service</th>
          <th>Could&rsquo;ve Been Better</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Tailscale for remote access</td>
          <td>Took trial-and-error to route Plex properly</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="status-after-phase-4">Status After Phase 4<a hidden class="anchor" aria-hidden="true" href="#status-after-phase-4">#</a></h2>
<ul>
<li>Plex, Minecraft reachable externally over HTTPS</li>
<li>All other services only accessible via LAN or Tailscale</li>
<li>Firewall restricts all unexpected inbound traffic</li>
<li>Remote access works without port-forwarding hell</li>
<li>VPN tunnel validates torrent container IP on boot</li>
<li>Nothing is open to the public that shouldn&rsquo;t be</li>
</ul>
<hr>
<h1 id="phase-5-the-monitoring">Phase 5: The Monitoring<a hidden class="anchor" aria-hidden="true" href="#phase-5-the-monitoring">#</a></h1>
<p><small><em>&ldquo;I&rsquo;m always watching, Wazowski.&rdquo;</em></small></p>
<p>I wanted eyes on everything: uptime, disk space, container health, and remote access status.</p>
<hr>
<h2 id="monitoring-goals">Monitoring Goals<a hidden class="anchor" aria-hidden="true" href="#monitoring-goals">#</a></h2>
<ul>
<li>Track availability of every container (and the VM itself)</li>
<li>Detect VPN leaks or tunnel failures</li>
<li>Get alerts if Plex, Sonarr, or automation tools go offline</li>
<li>Keep historical logs for debugging</li>
</ul>
<hr>
<h2 id="homepage">Homepage<a hidden class="anchor" aria-hidden="true" href="#homepage">#</a></h2>
<p>Homepage acts as a simple self-hosted dashboard to:</p>
<ul>
<li>Display current container statuses</li>
<li>Link to all local services</li>
<li>Show system info (disk, uptime, memory)</li>
<li>I did try Heimdall but I prefer Homepage&rsquo;s YAML-based config</li>
</ul>
<p>Mounted at <code>/mnt/ssd-linux/docker/homepage/</code>, the config is stored in:</p>
<pre><code>
/mnt/ssd-linux/docker/homepage/
├── config/
│   ├── services.yaml
│   ├── bookmarks.yaml
│   ├── settings.yaml
│   └── widgets.yaml
</code></pre>
<p>Accessible at <code>http://192.168.0.X:3005</code></p>
<p><img alt="caption" loading="lazy" src="/images/homelab/homepage.png"></p>
<hr>
<h2 id="uptime-kuma">Uptime Kuma<a hidden class="anchor" aria-hidden="true" href="#uptime-kuma">#</a></h2>
<p>Uptime Kuma tracks container availability via:</p>
<ul>
<li>TCP port probes</li>
<li>HTTP endpoint checks</li>
<li>API integrations (e.g. Plex, Sonarr, Radarr)</li>
<li>External DuckDNS URLs (to validate remote exposure)</li>
</ul>
<p>Accessible at <code>http://192.168.0.X:3001</code></p>
<p>Kuma database is stored at:</p>
<pre tabindex="0"><code>/mnt/ssd-linux/docker/uptime-kuma/kuma.db
</code></pre><hr>
<h2 id="monitored-services">Monitored Services<a hidden class="anchor" aria-hidden="true" href="#monitored-services">#</a></h2>
<table>
  <thead>
      <tr>
          <th>Service</th>
          <th>Type</th>
          <th>Target</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Plex</td>
          <td>HTTP</td>
          <td><code>/status/sessions?X-Plex-Token</code></td>
      </tr>
      <tr>
          <td>Sonarr</td>
          <td>API (JSON)</td>
          <td><code>/api/system/status</code></td>
      </tr>
      <tr>
          <td>Radarr</td>
          <td>API (JSON)</td>
          <td><code>/api/system/status</code></td>
      </tr>
      <tr>
          <td>qBittorrent</td>
          <td>TCP</td>
          <td>Port 8082</td>
      </tr>
      <tr>
          <td>Minecraft</td>
          <td>TCP</td>
          <td>Port 25565</td>
      </tr>
      <tr>
          <td>Filebrowser</td>
          <td>HTTP</td>
          <td>Port 8085</td>
      </tr>
      <tr>
          <td>Tailscale VPN</td>
          <td>Shell</td>
          <td><code>curl ifconfig.me</code> via Gluetun</td>
      </tr>
      <tr>
          <td>SSH</td>
          <td>TCP</td>
          <td>Port 22</td>
      </tr>
  </tbody>
</table>
<p><img alt="caption" loading="lazy" src="/images/homelab/kuma.png"></p>
<hr>
<h2 id="discord-notifications">Discord Notifications<a hidden class="anchor" aria-hidden="true" href="#discord-notifications">#</a></h2>
<p>Both Uptime Kuma, Plex watch status (Tautulli) and custom cronjobs send alerts via Discord webhook.</p>
<p>Sample mover script alert:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl -X POST -H <span style="color:#e6db74">&#34;Content-Type: application/json&#34;</span> -d <span style="color:#e6db74">&#39;{&#34;content&#34;:&#34;✅ Plex mover ran. 12 files moved.&#34;}&#39;</span> $DISCORD_WEBHOOK
</span></span></code></pre></div><p>Kuma also uses the same webhook to send alerts for downed or restored services.</p>
<p><img alt="caption" loading="lazy" src="/images/homelab/discord.png"></p>
<hr>
<h2 id="log-management">Log Management<a hidden class="anchor" aria-hidden="true" href="#log-management">#</a></h2>
<ul>
<li>All script logs are dumped to <code>/mnt/ssd-linux/logs/</code></li>
<li>Mover script log: <code>plex-mover.log</code></li>
<li>Minecraft backup log: <code>minecraft-backup.log</code></li>
<li>Logrotate compresses old logs daily and rotates after 7 days</li>
</ul>
<p>Sample crontab entry:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#ae81ff">0</span> * * * * /mnt/ssd-linux/scripts/plex-mover.sh &gt;&gt; /mnt/ssd-linux/logs/plex-mover.log 2&gt;&amp;<span style="color:#ae81ff">1</span>
</span></span></code></pre></div><hr>
<h2 id="lessons-from-phase-5">Lessons from Phase 5<a hidden class="anchor" aria-hidden="true" href="#lessons-from-phase-5">#</a></h2>
<table>
  <thead>
      <tr>
          <th>Win</th>
          <th>Problem Solved</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Using Kuma instead of Grafana stack</td>
          <td>Avoided overengineering</td>
      </tr>
      <tr>
          <td>External uptime for DuckDNS URLs</td>
          <td>Detected DNS propagation issues early</td>
      </tr>
      <tr>
          <td>Discord webhooks</td>
          <td>Instant visibility on automation status</td>
      </tr>
      <tr>
          <td>Homepage YAML config</td>
          <td>Easy to update, no GUI needed</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="status-after-phase-5">Status After Phase 5<a hidden class="anchor" aria-hidden="true" href="#status-after-phase-5">#</a></h2>
<ul>
<li>Every critical service monitored for uptime</li>
<li>Discord alerts sent on all failures or mover script completions</li>
<li>Web dashboards live at <code>3001</code> (Kuma) and <code>3005</code> (Homepage)</li>
<li>VPN IP validated and verified on boot</li>
<li>Historical logs available for analysis</li>
</ul>
<hr>
<h1 id="phase-6-the-audit">Phase 6: The Audit<a hidden class="anchor" aria-hidden="true" href="#phase-6-the-audit">#</a></h1>
<p><small><em>&ldquo;You either die a hero, or live long enough to run your own compliance check.&rdquo;</em></small></p>
<p>At this point, everything worked. But working isn’t the same as being reliable. That’s where the audit came in. A full walkthrough of every component, permission, port, and config.</p>
<p>This wasn’t just about validating what I had built. It was about <strong>documenting it</strong> so Future Me, or anyone else, could recover, debug, or upgrade the system without starting from scratch.</p>
<hr>
<h2 id="audit-objectives">Audit Objectives<a hidden class="anchor" aria-hidden="true" href="#audit-objectives">#</a></h2>
<ul>
<li>Ensure all mount points persist across reboots</li>
<li>Confirm correct container state, health, and restart behavior</li>
<li>Validate firewall rules and VPN isolation</li>
<li>Test remote access and DNS resolution</li>
<li>Backup critical configs and volumes</li>
<li>Generate a readable, re-runnable DRP (Disaster Recovery Plan)</li>
</ul>
<hr>
<h2 id="filesystem--mounts">Filesystem &amp; Mounts<a hidden class="anchor" aria-hidden="true" href="#filesystem--mounts">#</a></h2>
<p>All drives are mounted via <code>UUID</code> using <code>/etc/fstab</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>UUID<span style="color:#f92672">=</span>xxxxxx /mnt/plex-media ntfs-3g defaults <span style="color:#ae81ff">0</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>UUID<span style="color:#f92672">=</span>yyyyyy /mnt/ssd-linux ext4 defaults <span style="color:#ae81ff">0</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>UUID<span style="color:#f92672">=</span>zzzzzz /mnt/hdd ntfs-3g defaults <span style="color:#ae81ff">0</span> <span style="color:#ae81ff">0</span>
</span></span></code></pre></div><p>Checked using:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>lsblk
</span></span><span style="display:flex;"><span>mount | grep /mnt
</span></span><span style="display:flex;"><span>df -h
</span></span></code></pre></div><hr>
<h2 id="docker-healthcheck">Docker Healthcheck<a hidden class="anchor" aria-hidden="true" href="#docker-healthcheck">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker ps -a
</span></span><span style="display:flex;"><span>docker compose ps
</span></span><span style="display:flex;"><span>docker ps --filter <span style="color:#e6db74">&#39;status=restarting&#39;</span>
</span></span></code></pre></div><p>Confirmed:</p>
<ul>
<li>No containers stuck in restart loops</li>
<li>Plex, Sonarr, Radarr, Tautulli reachable on LAN</li>
<li>Config directories are writable and not root-owned</li>
<li>Gluetun VPN container routes traffic securely</li>
</ul>
<hr>
<h2 id="vpn--network-verification">VPN + Network Verification<a hidden class="anchor" aria-hidden="true" href="#vpn--network-verification">#</a></h2>
<p>Verified Gluetun’s tunnel IP:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker exec gluetun curl ifconfig.me
</span></span></code></pre></div><p>Also verified:</p>
<ul>
<li>Plex remote access works over Tailscale</li>
<li>Minecraft is externally reachable (via DuckDNS + port 25565)</li>
<li>Uptime Kuma shows correct up/down history for every container</li>
<li>No admin panels exposed to WAN</li>
</ul>
<hr>
<h2 id="firewall-rules-ufw">Firewall Rules (UFW)<a hidden class="anchor" aria-hidden="true" href="#firewall-rules-ufw">#</a></h2>
<p>Checked with:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo ufw status numbered
</span></span></code></pre></div><p>Confirmed:</p>
<ul>
<li>Only required ports are exposed externally</li>
<li>All other ports limited to LAN or Tailscale subnet</li>
</ul>
<hr>
<h2 id="backup-strategy">Backup Strategy<a hidden class="anchor" aria-hidden="true" href="#backup-strategy">#</a></h2>
<ul>
<li>Plex metadata stored under <code>/mnt/ssd-linux/plex/</code></li>
<li>Minecraft world backed up to tar.gz weekly</li>
<li>Kuma DB backed up manually and versioned</li>
<li>Docker volume configs backed up to <code>/mnt/hdd/docker-backup/</code> using rsync</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>rsync -avh /mnt/ssd-linux/ /mnt/hdd/docker-backup/
</span></span></code></pre></div><hr>
<h2 id="disaster-recovery-procedures">Disaster Recovery Procedures<a hidden class="anchor" aria-hidden="true" href="#disaster-recovery-procedures">#</a></h2>
<ol>
<li>Proxmox snapshot or fresh install</li>
<li>Restore VM or rebuild from clean ISO</li>
<li>Reattach drives, verify mount points</li>
<li>Reinstall Docker and Compose</li>
<li><code>cd ~/docker &amp;&amp; docker compose up -d</code></li>
<li>Validate Plex, automation tools, VPN, Kuma</li>
<li>Restore Minecraft</li>
<li>Resume cronjobs and verify Discord alerts</li>
</ol>
<hr>
<h2 id="lessons-from-phase-6">Lessons from Phase 6<a hidden class="anchor" aria-hidden="true" href="#lessons-from-phase-6">#</a></h2>
<table>
  <thead>
      <tr>
          <th>Good Habits</th>
          <th>Pain Avoided</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>fstab + UUIDs</td>
          <td>No boot hangs or missing drives</td>
      </tr>
      <tr>
          <td>Documented firewall rules</td>
          <td>Easy auditing + validation</td>
      </tr>
      <tr>
          <td>Verified VPN tunnel at boot</td>
          <td>No torrent leaks</td>
      </tr>
      <tr>
          <td>rsync for Docker configs</td>
          <td>Instant container recovery</td>
      </tr>
  </tbody>
</table>
<hr>
<h2 id="status-after-phase-6">Status After Phase 6<a hidden class="anchor" aria-hidden="true" href="#status-after-phase-6">#</a></h2>
<ul>
<li>Infrastructure fully audited and documented</li>
<li>Mounts and configs persist after full shutdown</li>
<li>Backups exist and are restorable</li>
<li>Containers and VPN behave as expected</li>
<li>Nothing is exposed that shouldn’t be</li>
<li>Disaster recovery validated on paper and in practice</li>
</ul>
<p>The stack is now <strong>reliable, recoverable, and repeatable</strong>.</p>
<p><code>Project complete.</code></p>
<hr>
<h1 id="final-thoughts">Final Thoughts<a hidden class="anchor" aria-hidden="true" href="#final-thoughts">#</a></h1>
<p>What started as a way to escape the chaos of modern streaming became a deep dive into infrastructure, automation, and self-reliance. Plex was just the spark. The real fire came from building the ecosystem, the orchestration, the failure handling, the why behind the how.</p>
<p>I’d written plenty of PowerShell before, mostly quick automations, task schedulers, file movers. But Linux is a different world. It demands precision. It punishes sloppiness. But it rewards patience, curiosity, and discipline. This was my first proper foray into that world.</p>
<p>It’s also personal. I don’t want my media held hostage by cloud outages or licensing changes. I want a system that runs on my terms. No ads. No logins. No “now available with premium.” Just content I care about, delivered how and when I want it.</p>
<p>I didn’t build this homelab out of necessity. I built it because I was tired of friction. Every script solved an annoyance. Every cronjob eliminated a chore. What began as a one-off Plex box became a platform, resilient, monitored, automated.</p>
<p>The wiring is messy. The screw’s still stripped. But the system works. It monitors itself. It alerts me. It’s documented. It’s fast. And above all: <strong>This one’s mine.</strong></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/docker/">Docker</a></li>
      <li><a href="http://localhost:1313/tags/plex/">Plex</a></li>
      <li><a href="http://localhost:1313/tags/proxmox/">Proxmox</a></li>
      <li><a href="http://localhost:1313/tags/tech/">Tech</a></li>
      <li><a href="http://localhost:1313/tags/linux/">Linux</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">RAW-TECH</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
